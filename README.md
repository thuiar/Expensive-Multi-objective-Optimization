# EMOP
> Paper List for Expensive Multi-objective Optimization 

## Table of Contents
:monkey:[Related Frameworks](#related-frameworks)   
:monkey:[Related Benchmarks](#related-benchmarks)   
:monkey:[Related Datasets](#related-datasets)  
:monkey:[Related Reviews](#related-reviews)  
:monkey:[Related Conferences and Journals](#related-conferences-and-journals)   
:monkey:[Multi-objective Bayesian Optimization](#multiobjective-bayesian-optimization) 

## Related Frameworks
- [BoTorch](https://botorch.org/)
- [GPyTorch](https://gpytorch.ai/)
- [COCO](https://github.com/numbbo/coco)
- [Geatpy](https://github.com/geatpy-dev/geatpy)
- [DEAP](https://github.com/DEAP/deap)
- [PlatEMO](https://github.com/BIMK/PlatEMO)
- [JMetal](https://github.com/jMetal/jMetal)
- [Paver](https://github.com/coin-or/Paver)


## Related Benchmarks
- [mDTLZ](https://ieeexplore.ieee.org/document/8372962)
- [UF](https://ojs.aaai.org/index.php/AAAI/article/view/10664) 
- [WFG](https://ieeexplore.ieee.org/document/5353656) 
- [DTLZ](https://www.cs.bham.ac.uk/~jdk/parego/)
- [Hyper-parameter Tuning](http://www2.imm.dtu.dk/pubdb/edoc/imm6284.pdf)\[[paper](http://www2.imm.dtu.dk/pubdb/edoc/imm6284.pdf), [code](https://github.com/rasmusbergpalm/DeepLearnToolbox)\]
- [RWCMOPs](https://www.sciencedirect.com/science/article/pii/S2210650221001231)
- [Real world Problems, RE](https://www.sciencedirect.com/science/article/pii/S1568494620300181), \[[Paper](https://ryojitanabe.github.io/reproblems/),[Code](https://github.com/happywhy/2021-RW-MOP)\]


## Related Reviews
- (2019) [Trends in Integration of Vision and Language Research: A Survey of Tasks, Datasets, and Methods](https://arxiv.org/abs/1907.09358)
- (2019) [Multimodal Machine Learning: A Survey and Taxonomy](https://ieeexplore.ieee.org/abstract/document/8269806/)  (:bulb::bulb::bulb:)
- (2019) [Deep Multimodal Representation Learning: A Survey](https://ieeexplore.ieee.org/abstract/document/8715409/)
- (2019) [Multimodal Intelligence: Representation Learning, Information Fusion, and Applications](https://arxiv.org/abs/1911.03977)
- (2018) [Multimodal Sentiment Analysis: Addressing Key Issues and Setting up Baselines](https://ieeexplore.ieee.org/abstract/document/8636432/) 
- (2016) [Chinese Textual Sentiment Analysis: Datasets, Resources and Tools](https://www.aclweb.org/anthology/C16-3002.pdf)
- (2017) [A review of affective computing: From unimodal analysis to multimodal fusion](https://www.sciencedirect.com/science/article/pii/S1566253517300738)
- (2017) [A survey of multimodal sentiment analysis](https://www.sciencedirect.com/science/article/pii/S0262885617301191) 
- (2013) [Representation Learning: A Review and New Perspectives](https://ieeexplore.ieee.org/abstract/document/6472238/)
- (2005) [Multimodal approaches for emotion recognition: a survey](https://www.spiedigitallibrary.org/conference-proceedings-of-spie/5670/0000/Multimodal-approaches-for-emotion-recognition-a-survey/10.1117/12.600746.short)

## Related Conferences and Journals
### Coferences
[ACL & EMNLP & NAACL & CoLing](https://www.aclweb.org/anthology/), 
[AAAI](https://www.aaai.org/Library/AAAI/aaai-library.php), 
[IJCAI](https://www.ijcai.org/proceedings/2019/), 
[ICLR](https://openreview.net/group?id=ICLR.cc/2019/Conference), 
[ICML](https://icml.cc/Conferences/2018/Schedule), 
[NeurIPS](https://nips.cc/Conferences/2018/Schedule?type=Poster), 
[ICMI](https://www.icmi.com/), 
[ACM-MM](https://2020.acmmm.org/)

### Journals
[IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE](),
[IEEE Transactions on Multimedia](), 
[IEEE Transactions on Affective Computing](), 
[Knowledge-based System](), 
[Information Fusion](), 
[IEEE Access](), 
[IEEE Intelligent Systems](), 


## Multimodal Sentiment Analysis
---
### Association for Computational Linguistics (ACL)
- (2020) [Integrating Multimodal Information in Large Pretrained Transformers](https://www.aclweb.org/anthology/2020.acl-main.214/) \[[code](https://github.com/WasifurRahman/BERT_multimodal_transformer)\]
- (2020) [A Recipe for Creating Multimodal Aligned Datasets for Sequential Tasks Angela](https://www.aclweb.org/anthology/2020.acl-main.440/)
- (2020) [Sentiment and Emotion help Sarcasm? A Multi-task Learning Framework for Multi-Modal Sarcasm, Sentiment and Emotion Analysis](https://www.aclweb.org/anthology/2020.acl-main.401/) \[[code](http://www.iitp.ac.in/˜ai-nlp-ml/resources.html)\]
- (2020) [CH-SIMS: A Chinese Multimodal Sentiment Analysis Dataset with Fine-grained Annotations of Modality](https://www.aclweb.org/anthology/2020.acl-main.343/) \[[code](https://github.com/thuiar/MMSA)\]
- (2020 Workshop) [A Transformer-based joint-encoding for Emotion Recognition and Sentiment Analysis](https://www.aclweb.org/anthology/2020.challengehml-1.1.pdf) \[[code](https://www.aclweb.org/anthology/2020.challengehml-1.1.pdf)\]
- (2020 Workshop) [Low Rank Fusion based Transformers for Multimodal Sequences](https://www.aclweb.org/anthology/2020.challengehml-1.4.pdf)
- (2019) [Modality-based Factorization for Multimodal Fusion](https://www.aclweb.org/anthology/W19-4331.pdf)
- (2019) [Divide, Conquer and Combine: Hierarchical Feature Fusion Network with Local and Global Perspectives for Multimodal Affective Computing](https://www.aclweb.org/anthology/P19-1046.pdf) 
- (2019) [Multimodal and Multi­view Models for Emotion Recognition](https://www.aclweb.org/anthology/P19-1095.pdf) \[[code](https://github.com/tzirakis/Multimodal-Emotion-Recognition)\]
- (2019) [Multimodal Transformer for Unaligned Multimodal Language Sequences](https://www.aclweb.org/anthology/P19-1656.pdf) \[[code](https://github.com/yaohungt/Multimodal-Transformer)\]
- (2019) [Contextual Inter-modal Attention for Multi-modal Sentiment Analysis](https://www.aclweb.org/anthology/D18-1382.pdf) \[[code](https://github.com/soujanyaporia/contextual-multimodal-fusion)\]
- (2019) [Towards Multimodal Sarcasm Detection (An Obviously Perfect Paper)](https://www.aclweb.org/anthology/P19-1455.pdf) \[[code](https://github.com/soujanyaporia/MUStARD)\]
- (2018) [Getting the subtext without the text: Scalable multimodal sentiment classification from visual and acoustic modalities](https://www.aclweb.org/anthology/W18-3301.pdf)
- (2018) [Recognizing Emotions in Video Using Multimodal DNN Feature Fusion](https://www.aclweb.org/anthology/W18-3302.pdf) \[[code](https://github.com/rhoposit/MultimodalDNN)\]
- (2018) [Efficient Low-rank Multimodal Fusion with Modality-Specific Factors](https://www.aclweb.org/anthology/P18-1209.pdf) \[[code](https://github.com/Justin1904/Low-rank-Multimodal-Fusion)\]
- (2018) [Multimodal Affective Analysis Using Hierarchical Attention Strategy with Word-Level Alignment](https://www.aclweb.org/anthology/P18-1207.pdf)
- (2018) [Multimodal Relational Tensor Network for Sentiment and Emotion Classification](https://www.aclweb.org/anthology/W18-3303.pdf)
- (2018) [Sentiment Analysis using Imperfect Views from Spoken Language and Acoustic Modalities](https://www.aclweb.org/anthology/W18-3305.pdf)
- (2018) [Seq2Seq2Sentiment: Multimodal Sequence to Sequence Models for Sentiment Analysis](https://www.aclweb.org/anthology/W18-3308.pdf)
- (2018) [Convolutional Attention Networks for Multimodal Emotion Recognition from Speech and Text Data](https://www.aclweb.org/anthology/W18-3304.pdf)
- (2018) [DNN Multimodal Fusion Techniques for Predicting Video Sentiment](https://www.aclweb.org/anthology/W18-3309.pdf)
- (2017) [Context-Dependent Sentiment Analysis in User-Generated Videos](https://www.aclweb.org/anthology/P17-1081.pdf) \[[code](https://github.com/soujanyaporia/contextual-utterance-level-multimodal-sentiment-analysis)\]
- (2017) [Multimodal Machine Learning: Integrating Language, Vision and Speech](https://www.aclweb.org/anthology/P17-5002.pdf)

### Empirical Methods in Natural Language Processing (EMNLP)
- (2020) [Dual Low-Rank Multimodal Fusion](https://www.aclweb.org/anthology/2020.findings-emnlp.35.pdf)
- (2020) [MAF: Multimodal Alignment Framework for Weakly-Supervised Phrase Grounding](https://www.aclweb.org/anthology/2020.emnlp-main.159.pdf) 
- (2020) [Does my multimodal model learn cross-modal interactions? It’s harder to tell than you might think!](https://www.aclweb.org/anthology/2020.emnlp-main.62/) \[[code](https://github.com/jmhessel/EMAP_EMNLP2020)\]
- (2020) [Multistage Fusion with Forget Gate for Multimodal Summarization in Open-Domain Videos](https://www.aclweb.org/anthology/2020.emnlp-main.144.pdf) \[[code](https://github.com/forkarinda/MFN)\]
- (2020) [Multi-modal Multi-label Emotion Detection with Modality and Label Dependence](https://www.aclweb.org/anthology/2020.emnlp-main.291.pdf) \[[code](https://github.com/MANLP-suda/MMS2S)\]
- (2019) [Context­aware Interactive Attention for Multi­modal Sentiment and Emotion Analysis](https://www.iitp.ac.in/~ai-nlp-ml/papers/emnlp19-emotion.pdf) \[[code](https://github.com/DushyantChauhan/EMNLP-19-IIM/tree/master)\]
- (2018) [Associative Multichannel Autoencoder for Multimodal Word Representation](https://www.aclweb.org/anthology/D18-1011.pdf) \[[code](https://github.com/wangshaonan/Associative-multichannel-autoencoder)\]
- (2018) [Contextual Inter-­modal Attention for Multi­modal Sentiment Analysis](https://www.aclweb.org/anthology/D18-1382.pdf) \[[code](https://github.com/soujanyaporia/contextual-multimodal-fusion)\]
- (2018) [Importance of Self­Attention for Sentiment Analysis](https://www.aclweb.org/anthology/W18-5429.pdf)
- (2018) [Improving Multi­-label Emotion Classification via Sentiment Classification with Dual Attention Transfer Network](https://www.aclweb.org/anthology/D18-1137.pdf)
- (2017) [Tensor Fusion Network for Multimodal Sentiment Analysis](https://www.aclweb.org/anthology/D17-1115.pdf) \[[code](https://github.com/A2Zadeh/TensorFusionNetwork)\]
- (2015) [Deep Convolutional Neural Network Textual Features and Multiple Kernel Learning for Utterance­level Multimodal Sentiment Analysis](https://www.aclweb.org/anthology/D15-1303.pdf)

### North American Chapter of the Association for Computational Linguistics (NAACL)
- (2019) [Strong and Simple Baselines for Multimodal Utterance Embeddings](https://www.aclweb.org/anthology/N19-1267.pdf) \[[code](https://github.com/yaochie/multimodal-baselines)\]
- (2019) [Quantifiers in a Multimodal World: Hallucinating Vision with Language and Sound](https://www.aclweb.org/anthology/W19-2912.pdf)
- (2019) [Multi­task Learning for Multi­modal Emotion Recognition and Sentiment Analysis](https://www.aclweb.org/anthology/N19-1034.pdf) \[[code](https://github.com/DushyantChauhan/NAACL-19-CIM)\]
- (2018) [Multimodal Emoji Prediction](https://www.aclweb.org/anthology/N18-2107.pdf)
- (2018) [Sentiment Analysis: It’s Complicated!](https://www.aclweb.org/anthology/N18-1171.pdf) \[[code](https://github.com/networkdynamics/mcgill-tsa)\]
- (2016) [Bridge Correlational Neural Networks for Multilingual Multimodal Representation Learning](https://www.aclweb.org/anthology/N16-1021.pdf)

### International Conference on Computational Linguistics （CoLing）
- (2018) [Hybrid Attention based Multimodal Network for Spoken Language Classification](https://www.aclweb.org/anthology/C18-1201.pdf)
- (2018) [Learning Emotion­enriched Word Representations](https://www.aclweb.org/anthology/C18-1081.pdf)
- (2018) [Emotion Detection and Classification in a Multigenre Corpus with Joint Multi­Task Deep Learning](https://www.aclweb.org/anthology/C18-1246.pdf)
- (2016) [Multimodal Mood Classification ­ A Case Study of Differences in Hindi and Western Songs](https://www.aclweb.org/anthology/C16-1186.pdf)

### Association for the Advancement of Artificial Intelligence (AAAI)
- (2019) [VistaNet: Visual Aspect Attention Network for Multimodal Sentiment Analysis](https://aaai.org/ojs/index.php/AAAI/article/view/3799) \[[code](https://github.com/PreferredAI/vista-net)\]
- (2019) [Multi-Interactive Memory Network for Aspect Based Multimodal Sentiment Analysis](https://aaai.org/ojs/index.php/AAAI/article/view/3807) \[[code](https://github.com/xunan0812/MIMN)\]
- (2019) [An Efficient Approach to Informative Feature Extraction from Multimodal Data](https://aaai.org/ojs/index.php/AAAI/article/view/4464)
- (2019) [Words Can Shift: Dynamically Adjusting Word Representations Using Nonverbal Behaviors](https://www.aaai.org/ojs/index.php/AAAI/article/download/4706/4584) \[[code](https://github.com/victorywys/RAVEN)\]
- (2019) [Found in Translation: Learning Robust Joint Representations by Cyclic Translations Between Modalities](https://wvvw.aaai.org/ojs/index.php/AAAI/article/download/4666/4544) 
- (2019) [Modality to Modality Translation: An Adversarial Representation Learning and Graph Fusion Network for Multimodal Fusion](https://www.aaai.org/Papers/AAAI/2020GB/AAAI-MaiS.10285.pdf)
- (2018) [Learning Multimodal Word Representation via Dynamic Fusion Methods](https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/download/16167/16164)
- (2018) [Predicting Depression Severity by Multi-Modal Feature Engineering and Fusion](https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/download/16415/16511)
- (2018) [Memory Fusion Network for Multi-View Sequential Learning](https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewPDFInterstitial/17341/16122) \[[code](https://github.com/pliang279/MFN)\]
- (2018) [Inferring Emotion from Conversational Voice Data: A Semi-Supervised Multi-Path Generative Neural Network Approach](https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewPDFInterstitial/17236/15735) 
- (2017) [Multimodal Fusion of EEG and Musical Features in Music-Emotion Recognition](https://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14831/14237)
- (2016) [Personalized Microblog Sentiment Classification via Multi-Task Learning](https://aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/12031/12061)

### Computer Vision and Pattern Recognition (CVPR)
- (2020) [What Makes Training Multi-modal Classification Networks Hard?](http://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_What_Makes_Training_Multi-Modal_Classification_Networks_Hard_CVPR_2020_paper.pdf)

### International Joint Conferences on Artificial Intelligence (IJCAI)
- (2019) [Success Prediction on Crowdfunding with Multimodal Deep Learning](https://www.ijcai.org/proceedings/2019/0299.pdf)
- (2019) [AttnSense: Multi-level Attention Mechanism For Multimodal Human Activity Recognition](https://www.ijcai.org/Proceedings/2019/0431.pdf) \[[code](https://github.com/zzpustc/ML-Application-on-Network)\]
- (2019) [DeepCU: Integrating both Common and Unique Latent Information for Multimodal Sentiment Analysis](https://www.ijcai.org/Proceedings/2019/0503.pdf) \[[code](https://github.com/sverma88/DeepCU-IJCAI19)\]
- (2019) [Adapting BERT for Target-Oriented Multimodal Sentiment Classification](https://www.ijcai.org/Proceedings/2019/0751.pdf) \[[code](https://github.com/jefferyYu/TomBERT)\]
- (2019) [Multi-view Clustering via Late Fusion Alignment Maximization](https://www.ijcai.org/proceedings/2019/0524.pdf)
- (2019) [Towards Discriminative Representation Learning for Speech Emotion Recognition](https://www.ijcai.org/Proceedings/2019/0703.pdf) \[[code](https://github.com/thuhcsi/IJCAI2019-DRL4SER)\]

### International Conference on Learning Representations (ICLR)
- (2019) [Learning Factorized Multimodal Representations](https://openreview.net/forum?id=rygqqsA9KX) \[[code](https://github.com/pliang279/factorized)\]

### Neural Information Processing Systems (NeurIPS)
- (2018) [Learning Robust Joint Representations for Multimodal Sentiment Analysis](https://openreview.net/pdf?id=rkgx8x1js7)

### Other
- (2020) [Factorized Multimodal Transformer for Multimodal Sequential Learning](https://openreview.net/pdf?id=BJxD11HFDS) \[[code](https://github.com/A2Zadeh/Factorized-Multimodal-Transformer)\]
- (2020) [OmniNet: A unified architecture for multi-modal multi-task learning](https://openreview.net/pdf?id=HJgdo6VFPH) \[[code](https://github.com/subho406/OmniNet), [Review](https://openreview.net/forum?id=HJgdo6VFPH)\]

### IEEE International Conference on Data Mining (ICDM)
- (2020) [Deep-HOSeq: Deep Higher Order Sequence Fusion for Multimodal Sentiment Analysis](https://arxiv.org/abs/2010.08218) \[[code](https://github.com/sverma88/Deep-HOSeq--ICDM-2020)\]

<!-- #### International Conference on Machine Learning (ICML) -->

### International Conference on Multimodal Interaction (ICMI)
- (2019) [Multimodal Sentiment Analysis with Word-Level Fusion and Reinforcement Learning](https://dl.acm.org/doi/pdf/10.1145/3136755.3136801)


## Emotion Recognition in Conversations
---
### Association for the Advancement of Artificial Intelligence (AAAI)
- (2019) [DialogueRNN: An Attentive RNN for Emotion Detection in Conversations](https://www.aaai.org/ojs/index.php/AAAI/article/download/4657/4535) \[[code](https://github.com/SenticNet/conv-emotion)\]

### Empirical Methods in Natural Language Processing (EMNLP)
- (2019) [DialogueGCN: A Graph Convolutional Neural Network for Emotion Recognition in Conversation](https://www.aclweb.org/anthology/D19-1015/) \[[code](https://github.com/SenticNet/conv-emotion)\]
- (2018) [ICON: Interactive Conversational Memory Network for Multimodal Emotion Detection](https://www.aclweb.org/anthology/D18-1280.pdf) \[[code](https://github.com/SenticNet/conv-emotion)\]

### North American Chapter of the Association for Computational Linguistics (NAACL)
- (2019) [HiGRU: Hierarchical Gated Recurrent Units for Utterance-level Emotion Recognition](https://www.aclweb.org/anthology/N19-1037.pdf) \[[code](https://github.com/wxjiao/HiGRUs)\]
- (2018) [Conversational Memory Network for Emotion Recognition in Dyadic Dialogue Videos](https://www.aclweb.org/anthology/N18-1193.pdf) \[[code](https://github.com/SenticNet/conv-emotion)\]

### ACM International Conference on Multimedia (ACM MM)
- (2018) [Inferring User Emotive State Changes in Realistic Human-Computer Conversational Dialogs](https://dl.acm.org/doi/pdf/10.1145/3240508.3240575)
